"""
EvoAlpha OS - ä¸ªè‚¡Kçº¿æ•°æ®é‡‡é›†å™¨
é‡‡é›†è‚¡ç¥¨çš„æ—¥çº§è¡Œæƒ…æ•°æ®
"""

import time
import datetime
import pandas as pd
import akshare as ak
from sqlalchemy import text, inspect
from datetime import timedelta

# å…¬å…±å·¥å…·å¯¼å…¥
from data_job.common import setup_network_emergency_kit, setup_backend_path, setup_logger

# åŸºç±»å¯¼å…¥
from data_job.core.base_collector import BaseCollector

from app.core.database import get_engine

# è·¯å¾„å’Œç½‘ç»œåˆå§‹åŒ–
setup_backend_path()
setup_network_emergency_kit()

# Loggeré…ç½®
logger = setup_logger(__name__)


class StockKlineCollector(BaseCollector):
    """ä¸ªè‚¡Kçº¿æ•°æ®é‡‡é›†å™¨"""

    def __init__(self):
        super().__init__(
            collector_name="stock_kline",
            request_timeout=30,
            request_delay=0.01,
            max_retries=3
        )
        self.engine = get_engine()
        self.table_name = "stock_daily_prices"
        self.batch_size = 500

    def _init_table(self):
        """åˆå§‹åŒ– daily_prices è¡¨ç»“æ„"""
        inspector = inspect(self.engine)
        if not inspector.has_table(self.table_name):
            logger.info(f"ğŸ› ï¸ åˆ›å»ºè¡¨ {self.table_name}...")
            with self.engine.begin() as conn:
                conn.execute(text(f"""
                    CREATE TABLE {self.table_name} (
                        symbol VARCHAR(20),
                        trade_date DATE,
                        open FLOAT,
                        close FLOAT,
                        high FLOAT,
                        low FLOAT,
                        volume FLOAT,
                        amount FLOAT,
                        pct_chg FLOAT,
                        turnover_rate FLOAT,
                        PRIMARY KEY (symbol, trade_date)
                    );
                """))
                conn.execute(text(f"CREATE INDEX IF NOT EXISTS idx_kline_symbol ON {self.table_name} (symbol);"))
                conn.execute(text(f"CREATE INDEX IF NOT EXISTS idx_kline_date ON {self.table_name} (trade_date);"))

    def get_stock_list(self):
        """è·å–è‚¡ç¥¨åå•ï¼šä¼˜å…ˆæŸ¥æ•°æ®åº“"""
        logger.info("ğŸ“‹ æ­£åœ¨è·å–å¾…æ›´æ–°çš„è‚¡ç¥¨åå•...")
        # 1. å°è¯•ä»ã€æ¿å—æ˜ å°„è¡¨ã€‘è¯»å–
        try:
            with self.engine.connect() as conn:
                df = pd.read_sql(text("SELECT DISTINCT symbol, name FROM stock_sector_map"), conn)
            if not df.empty:
                logger.info(f"âœ… ä» stock_sector_map è·å–åˆ° {len(df)} åªè‚¡ç¥¨")
                return df[df['symbol'].astype(str).str.match(r'^(00|30|60|68)')].to_dict('records')
        except Exception:
            pass

        # 2. å°è¯•ä»ã€åŸºç¡€ä¿¡æ¯è¡¨ã€‘è¯»å–
        try:
            with self.engine.connect() as conn:
                df = pd.read_sql(text("SELECT symbol, name FROM stock_info"), conn)
            if not df.empty:
                logger.info(f"âœ… ä» stock_info è·å–åˆ° {len(df)} åªè‚¡ç¥¨")
                return df.to_dict('records')
        except Exception:
            pass

        # 3. æœ€åè”ç½‘è·å–
        for i in range(3):
            try:
                df = self._retry_call(ak.stock_zh_a_spot_em, max_retries=2)
                if df is not None:
                    df = df[['ä»£ç ', 'åç§°']].rename(columns={'ä»£ç ': 'symbol', 'åç§°': 'name'})
                    return df[df['symbol'].astype(str).str.match(r'^(00|30|60|68)')].to_dict('records')
            except Exception:
                time.sleep(2)
        return []

    def get_last_dates(self):
        """è·å–å¢é‡æ›´æ–°è¿›åº¦"""
        try:
            query = text(f"SELECT symbol, MAX(trade_date) as last_date FROM {self.table_name} GROUP BY symbol")
            with self.engine.connect() as conn:
                df = pd.read_sql(query, conn)
            if df.empty:
                return {}
            return dict(zip(df['symbol'], pd.to_datetime(df['last_date']).dt.date))
        except:
            return {}

    def _bulk_save_kline(self, df_list):
        """æ‰¹é‡å­˜å…¥æ•°æ®åº“"""
        if not df_list:
            return
        try:
            final_df = pd.concat(df_list, ignore_index=True)
            with self.engine.begin() as conn:
                final_df.to_sql(self.table_name, conn, if_exists='append', index=False, method='multi', chunksize=1000)
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡å†™å…¥å¤±è´¥: {e}")

    def run(self):
        """ä¸»æ‰§è¡Œå…¥å£"""
        self.log_collection_start()
        logger.info("ğŸš€ [Kçº¿] å¯åŠ¨ä¸ªè‚¡è¡Œæƒ…åŒæ­¥...")
        self._init_table()

        try:
            # å¥åº·æ£€æŸ¥
            self._health_check()
        except Exception as e:
            logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            self.log_collection_end(False, str(e))
            return

        stock_list = self.get_stock_list()
        if not stock_list:
            logger.error("âŒ æœªè·å–åˆ°è‚¡ç¥¨åˆ—è¡¨")
            self.log_collection_end(False, "æ— è‚¡ç¥¨åˆ—è¡¨")
            return

        existing_records = self.get_last_dates()
        DEFAULT_START_DATE = "20230101"
        today = datetime.date.today()
        total = len(stock_list)

        collected_data = []
        BATCH_SIZE = self.batch_size

        logger.info(f"ğŸ“Š å‡†å¤‡å¤„ç† {total} åªè‚¡ç¥¨...")

        for i, stock in enumerate(stock_list):
            code = stock['symbol']
            name = stock['name']

            last_date = existing_records.get(code)
            if last_date:
                if last_date >= today:
                    continue
                start_date_str = (last_date + timedelta(days=1)).strftime("%Y%m%d")
            else:
                start_date_str = DEFAULT_START_DATE

            end_date_str = today.strftime("%Y%m%d")
            if start_date_str > end_date_str:
                continue

            if i % 10 == 0:
                print(f"[{i+1}/{total}] åŒæ­¥è¿›åº¦: {code} {name} ...", end="\r")

            try:
                # ä½¿ç”¨åŸºç±»çš„é‡è¯•æœºåˆ¶
                df = self._retry_call(
                    ak.stock_zh_a_hist,
                    symbol=code, period="daily", start_date=start_date_str,
                    end_date=end_date_str, adjust="qfq"
                )

                if df is None or df.empty:
                    continue

                rename_dict = {
                    'æ—¥æœŸ': 'trade_date', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close',
                    'æœ€é«˜': 'high', 'æœ€ä½': 'low', 'æˆäº¤é‡': 'volume',
                    'æˆäº¤é¢': 'amount', 'æ¶¨è·Œå¹…': 'pct_chg', 'æ¢æ‰‹ç‡': 'turnover_rate'
                }
                df = df.rename(columns=rename_dict)
                df['symbol'] = code

                for col in ['open', 'close', 'high', 'low', 'volume', 'amount', 'pct_chg', 'turnover_rate']:
                    if col not in df.columns:
                        df[col] = None

                df['trade_date'] = pd.to_datetime(df['trade_date']).dt.date
                save_df = df[['symbol', 'trade_date', 'open', 'close', 'high', 'low', 'volume', 'amount', 'pct_chg', 'turnover_rate']]

                collected_data.append(save_df)

                if len(collected_data) >= BATCH_SIZE:
                    self._bulk_save_kline(collected_data)
                    collected_data = []

            except Exception as e:
                logger.debug(f"é‡‡é›† {code} å¤±è´¥: {e}")
                time.sleep(0.2)

        if collected_data:
            self._bulk_save_kline(collected_data)

        logger.info(f"\nâœ… ä¸ªè‚¡ K çº¿åŒæ­¥å®Œæˆï¼")
        self.log_collection_end(True, f"å¤„ç† {total} åªè‚¡ç¥¨")


if __name__ == "__main__":
    collector = StockKlineCollector()
    collector.run()
