"""
EvoAlpha OS - å®è§‚ç»æµæ•°æ®é‡‡é›†å™¨
é‡‡é›† GDPã€CPIã€PMI ç­‰å®è§‚æ•°æ®
"""

import time
import pandas as pd
import akshare as ak
from sqlalchemy import text
from datetime import datetime

# å…¬å…±å·¥å…·å¯¼å…¥
from data_job.common import setup_network_emergency_kit, setup_backend_path, setup_logger

# åŸºç±»å¯¼å…¥
from data_job.core.base_collector import BaseCollector

from app.core.database import get_active_engines

# è·¯å¾„å’Œç½‘ç»œåˆå§‹åŒ–
setup_backend_path()
setup_network_emergency_kit()

# Loggeré…ç½®
logger = setup_logger(__name__)


class MacroDataCollector(BaseCollector):
    """å®è§‚ç»æµæ•°æ®é‡‡é›†å™¨"""

    def __init__(self):
        super().__init__(
            collector_name="macro_data",
            request_timeout=30,
            request_delay=0.5,
            max_retries=3
        )
        self.engines = get_active_engines()
        self.table_name = "macro_indicators"

    def _init_table(self):
        """åˆå§‹åŒ–å®è§‚æŒ‡æ ‡è¡¨"""
        for mode, engine in self.engines:
            logger.info(f"ğŸ› ï¸  [{mode}] åˆ›å»ºå®è§‚æŒ‡æ ‡è¡¨...")
            try:
                with engine.begin() as conn:
                    conn.execute(text(f"""
                        CREATE TABLE IF NOT EXISTS {self.table_name} (
                            indicator_name VARCHAR(50),
                            indicator_code VARCHAR(20),
                            period VARCHAR(20),
                            value FLOAT,
                            forecast_value FLOAT,
                            previous_value FLOAT,
                            unit VARCHAR(20),
                            publish_date DATE,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            PRIMARY KEY (indicator_code, period)
                        );
                    """))
                    try:
                        conn.execute(text(f"CREATE INDEX IF NOT EXISTS idx_macro_date ON {self.table_name} (publish_date);"))
                    except Exception:
                        pass
                    try:
                        conn.execute(text(f"CREATE INDEX IF NOT EXISTS idx_macro_name ON {self.table_name} (indicator_name);"))
                    except Exception:
                        pass
                    logger.info(f"âœ… [{mode}] å®è§‚æŒ‡æ ‡è¡¨åˆ›å»ºæˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ [{mode}] åˆ›å»ºå®è§‚æŒ‡æ ‡è¡¨å¤±è´¥: {e}")

    def fetch_gdp(self):
        """è·å–GDPæ•°æ®"""
        try:
            # ä½¿ç”¨åŸºç±»çš„é‡è¯•æœºåˆ¶
            df = self._retry_call(ak.macro_china_gdp)
            if df.empty:
                return None

            # æ•°æ®æ¸…æ´—
            df = df.rename(columns={
                'å­£åº¦': 'period',
                'å›½å†…ç”Ÿäº§æ€»å€¼-ç»å¯¹å€¼': 'value',
            })
            df['indicator_name'] = 'GDP'
            df['indicator_code'] = 'GDP'
            df['unit'] = 'äº¿å…ƒ'
            df['forecast_value'] = None
            df['previous_value'] = None

            # å­£åº¦æ ¼å¼è½¬æ¢ï¼š2025å¹´ç¬¬1-4å­£åº¦ -> 2025-Q4
            df['period'] = df['period'].str.replace('å¹´ç¬¬1-4å­£åº¦', '-Q4').str.replace('å¹´ç¬¬1-3å­£åº¦', '-Q3').str.replace('å¹´ç¬¬1-2å­£åº¦', '-Q2').str.replace('å¹´ç¬¬1å­£åº¦', '-Q1')

            # è½¬æ¢ä¸ºæ—¥æœŸï¼ˆå­£åº¦æœ«ï¼‰
            df['publish_date'] = pd.to_datetime(df['period'], format='%Y-Q%m') + pd.offsets.QuarterEnd(0)

            df = df[['indicator_name', 'indicator_code', 'period', 'value', 'forecast_value', 'previous_value', 'unit', 'publish_date']]
            logger.info(f"  âœ… GDP: {len(df)} æ¡æ•°æ®")
            return df

        except Exception as e:
            logger.error(f"âŒ GDPæ•°æ®è·å–å¤±è´¥: {e}")
            return None

    def fetch_cpi(self):
        """è·å–CPIæ•°æ®"""
        try:
            # ä½¿ç”¨åŸºç±»çš„é‡è¯•æœºåˆ¶
            df = self._retry_call(ak.macro_china_cpi_yearly)
            if df.empty:
                return None

            # æ•°æ®æ¸…æ´—ï¼šæ˜ å°„å®é™…å­—æ®µ
            df = df.rename(columns={
                'å•†å“': 'indicator_name',
                'æ—¥æœŸ': 'publish_date',
                'ä»Šå€¼': 'value',
                'é¢„æµ‹å€¼': 'forecast_value',
                'å‰å€¼': 'previous_value'
            })
            df['indicator_code'] = 'CPI'
            df['unit'] = '%'
            df['period'] = pd.to_datetime(df['publish_date']).dt.strftime('%Y-%m-%d')

            # åˆ é™¤é‡å¤æ•°æ®ï¼ˆä¿ç•™æœ€æ–°çš„ï¼‰
            df = df.drop_duplicates(subset=['period'], keep='last')

            # åªä¿ç•™éœ€è¦çš„åˆ—
            df = df[['indicator_name', 'indicator_code', 'period', 'value', 'forecast_value', 'previous_value', 'unit', 'publish_date']]
            logger.info(f"  âœ… CPI: {len(df)} æ¡æ•°æ®")
            return df

        except Exception as e:
            logger.error(f"âŒ CPIæ•°æ®è·å–å¤±è´¥: {e}")
            return None

    def fetch_pmi(self):
        """è·å–PMIæ•°æ®ï¼ˆåˆ¶é€ ä¸šé‡‡è´­ç»ç†æŒ‡æ•°ï¼‰"""
        try:
            # ä½¿ç”¨åŸºç±»çš„é‡è¯•æœºåˆ¶
            df = self._retry_call(ak.macro_china_pmi_yearly)
            if df.empty:
                return None

            # æ•°æ®æ¸…æ´—ï¼šæ˜ å°„å®é™…å­—æ®µ
            df = df.rename(columns={
                'å•†å“': 'indicator_name',
                'æ—¥æœŸ': 'publish_date',
                'ä»Šå€¼': 'value',
                'é¢„æµ‹å€¼': 'forecast_value',
                'å‰å€¼': 'previous_value'
            })
            df['indicator_code'] = 'PMI'
            df['unit'] = '%'
            df['period'] = pd.to_datetime(df['publish_date']).dt.strftime('%Y-%m-%d')

            # åˆ é™¤é‡å¤æ•°æ®ï¼ˆä¿ç•™æœ€æ–°çš„ï¼‰
            df = df.drop_duplicates(subset=['period'], keep='last')

            # åªä¿ç•™éœ€è¦çš„åˆ—
            df = df[['indicator_name', 'indicator_code', 'period', 'value', 'forecast_value', 'previous_value', 'unit', 'publish_date']]
            logger.info(f"  âœ… PMI: {len(df)} æ¡æ•°æ®")
            return df

        except Exception as e:
            logger.error(f"âŒ PMIæ•°æ®è·å–å¤±è´¥: {e}")
            return None

    def save_macro_data(self, all_data):
        """
        ä¿å­˜å®è§‚æ•°æ®

        Args:
            all_data: æ‰€æœ‰å®è§‚æ•°æ®çš„åˆ—è¡¨
        """
        if not all_data:
            logger.warning("âš ï¸  å®è§‚æ•°æ®ä¸ºç©º")
            return

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_data, ignore_index=True)

        for mode, engine in self.engines:
            try:
                with engine.begin() as conn:
                    # é€ä¸ªæŒ‡æ ‡åˆ é™¤æ—§æ•°æ®å¹¶æ’å…¥æ–°æ•°æ®
                    for indicator_code in combined_df['indicator_code'].unique():
                        df_indicator = combined_df[combined_df['indicator_code'] == indicator_code]

                        # åˆ é™¤æ—§æ•°æ®
                        conn.execute(text(f"""
                            DELETE FROM {self.table_name}
                            WHERE indicator_code = :indicator_code
                        """), {"indicator_code": indicator_code})

                        # æ’å…¥æ–°æ•°æ®ï¼ˆä½¿ç”¨ chunksize é¿å… SQLite å˜é‡é™åˆ¶ï¼‰
                        df_indicator.to_sql(self.table_name, conn, if_exists='append', index=False,
                                          method='multi', chunksize=100)

                    logger.info(f"âœ… [{mode}] ä¿å­˜ {len(combined_df)} æ¡å®è§‚æ•°æ®")

            except Exception as e:
                logger.error(f"âŒ [{mode}] ä¿å­˜å®è§‚æ•°æ®å¤±è´¥: {e}")

    def run(self):
        """æ‰§è¡Œå®è§‚æ•°æ®é‡‡é›†"""
        self.log_collection_start()
        logger.info("ğŸš€ å¼€å§‹é‡‡é›†å®è§‚æ•°æ®...")

        try:
            # å¥åº·æ£€æŸ¥
            self._health_check()
        except Exception as e:
            logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            self.log_collection_end(False, str(e))
            return

        # åˆå§‹åŒ–è¡¨
        self._init_table()

        # é‡‡é›†å„ç±»å®è§‚æ•°æ®
        all_data = []

        # GDP
        logger.info("ğŸ“Š é‡‡é›† GDP æ•°æ®...")
        gdp_data = self.fetch_gdp()
        if gdp_data is not None:
            all_data.append(gdp_data)

        time.sleep(1)

        # CPI
        logger.info("ğŸ“Š é‡‡é›† CPI æ•°æ®...")
        cpi_data = self.fetch_cpi()
        if cpi_data is not None:
            all_data.append(cpi_data)

        time.sleep(1)

        # PMI
        logger.info("ğŸ“Š é‡‡é›† PMI æ•°æ®...")
        pmi_data = self.fetch_pmi()
        if pmi_data is not None:
            all_data.append(pmi_data)

        # ä¿å­˜æ•°æ®
        if all_data:
            self.save_macro_data(all_data)
            total_count = sum(len(d) for d in all_data)
            logger.info(f"ğŸ‰ å®è§‚æ•°æ®é‡‡é›†å®Œæˆï¼Œå…± {total_count} æ¡")
            self.log_collection_end(True, f"é‡‡é›† {total_count} æ¡æ•°æ®")
        else:
            logger.error("âŒ æœªè·å–åˆ°ä»»ä½•å®è§‚æ•°æ®")
            self.log_collection_end(False, "æ— æ•°æ®è·å–")


if __name__ == "__main__":
    collector = MacroDataCollector()
    collector.run()
