"""
EvoAlpha OS - æ–°é—»èˆ†æƒ…æ•°æ®é‡‡é›†å™¨
é‡‡é›†è´¢ç»æ–°é—»å¹¶è¿›è¡Œè‚¡ç¥¨å…³è”å’Œæƒ…ç»ªåˆ†æ
"""

import time
import pandas as pd
import akshare as ak
from sqlalchemy import text
from datetime import datetime, timedelta, date
import re

# å…¬å…±å·¥å…·å¯¼å…¥
from data_job.common import setup_network_emergency_kit, setup_backend_path, setup_logger

# åŸºç±»å¯¼å…¥
from data_job.core.base_collector import BaseCollector

from app.core.database import get_active_engines

# è·¯å¾„å’Œç½‘ç»œåˆå§‹åŒ–
setup_backend_path()
setup_network_emergency_kit()

# Loggeré…ç½®
logger = setup_logger(__name__)


class NewsCollector(BaseCollector):
    """æ–°é—»èˆ†æƒ…æ•°æ®é‡‡é›†å™¨"""

    def __init__(self):
        super().__init__(
            collector_name="news",
            request_timeout=30,
            request_delay=1,
            max_retries=3
        )
        self.engines = get_active_engines()
        self.articles_table = "news_articles"
        self.relation_table = "news_stock_relation"

    def _init_tables(self):
        """åˆå§‹åŒ–æ–°é—»ç›¸å…³è¡¨"""
        for mode, engine in self.engines:
            logger.info(f"ğŸ› ï¸  [{mode}] åˆ›å»ºæ–°é—»è¡¨...")
            try:
                with engine.begin() as conn:
                    conn.execute(text(f"""
                        CREATE TABLE IF NOT EXISTS {self.articles_table} (
                            article_id VARCHAR(50) PRIMARY KEY,
                            title VARCHAR(200),
                            content TEXT,
                            source VARCHAR(50),
                            publish_time TIMESTAMP,
                            url VARCHAR(500),
                            sentiment_type VARCHAR(10),
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        );
                    """))

                    conn.execute(text(f"""
                        CREATE TABLE IF NOT EXISTS {self.relation_table} (
                            article_id VARCHAR(50),
                            symbol VARCHAR(20),
                            relevance_score FLOAT,
                            sentiment_type VARCHAR(10),
                            PRIMARY KEY (article_id, symbol)
                        );
                    """))

                    conn.execute(text(f"CREATE INDEX IF NOT EXISTS idx_news_time ON {self.articles_table} (publish_time);"))
                    conn.execute(text(f"CREATE INDEX IF NOT EXISTS idx_news_symbol ON {self.relation_table} (symbol);"))

                    logger.info(f"âœ… [{mode}] æ–°é—»è¡¨åˆ›å»ºæˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ [{mode}] åˆ›å»ºæ–°é—»è¡¨å¤±è´¥: {e}")

    def fetch_news_em(self, date_str=None):
        """ä»ä¸œæ–¹è´¢å¯Œè·å–æ–°é—»"""
        try:
            # ä½¿ç”¨åŸºç±»çš„é‡è¯•æœºåˆ¶
            df = self._retry_call(ak.stock_news_em)

            if df.empty:
                logger.warning(f"âš ï¸  æ— æ–°é—»æ•°æ®")
                return None

            df = df.rename(columns={
                'æ–°é—»æ ‡é¢˜': 'title',
                'æ–°é—»å†…å®¹': 'content',
                'æ–‡ç« æ¥æº': 'source',
                'å‘å¸ƒæ—¶é—´': 'publish_time',
                'æ–°é—»é“¾æ¥': 'url',
            })

            df['article_id'] = df['url'].apply(lambda x: f"EM_{hash(x) % 10000000000:08d}")
            df['publish_time'] = pd.to_datetime(df['publish_time'])
            df['sentiment_type'] = 'neutral'

            logger.info(f"  âœ… ä¸œæ–¹è´¢å¯Œ: {len(df)} æ¡æ–°é—»")
            return df

        except Exception as e:
            logger.error(f"âŒ ä¸œæ–¹è´¢å¯Œæ–°é—»é‡‡é›†å¤±è´¥: {e}")
            return None

    def extract_stock_symbols(self, text):
        """ä»æ–‡æœ¬ä¸­æå–è‚¡ç¥¨ä»£ç """
        pattern = r'\b(00|30|60|68)\d{4}\b'
        matches = re.findall(pattern, text)
        symbols = list(set(matches))
        return symbols

    def analyze_sentiment(self, title, content):
        """ç®€å•çš„æƒ…ç»ªåˆ†æï¼ˆåŸºäºå…³é”®è¯ï¼‰"""
        positive_keywords = ['å¤§æ¶¨', 'ä¸Šæ¶¨', 'åˆ©å¥½', 'çªç ´', 'å¢é•¿', 'ç›ˆåˆ©', 'æ¶¨åœ',
                            'å›è´­', 'å¢æŒ', 'æ”¶è´­', 'ä¸šç»©', 'åˆ›æ–°é«˜', 'é¢†æ¶¨']

        negative_keywords = ['å¤§è·Œ', 'ä¸‹è·Œ', 'åˆ©ç©º', 'è·Œç ´', 'ä¸‹é™', 'äºæŸ', 'è·Œåœ',
                            'å‡æŒ', 'ä¸šç»©', 'åˆ›æ–°ä½', 'é¢†è·Œ', 'è°ƒæŸ¥', 'å¤„ç½š']

        text = title + ' ' + str(content)

        positive_count = sum(1 for kw in positive_keywords if kw in text)
        negative_count = sum(1 for kw in negative_keywords if kw in text)

        if positive_count > negative_count:
            return 'positive'
        elif negative_count > positive_count:
            return 'negative'
        else:
            return 'neutral'

    def save_news(self, df):
        """ä¿å­˜æ–°é—»åˆ°æ•°æ®åº“"""
        if df is None or df.empty:
            return

        for mode, engine in self.engines:
            try:
                with engine.begin() as conn:
                    articles_df = df[['article_id', 'title', 'content', 'source', 'publish_time', 'url', 'sentiment_type']]

                    article_ids = "', '".join([str(aid) for aid in articles_df['article_id']])
                    conn.execute(text(f"""
                        DELETE FROM {self.articles_table}
                        WHERE article_id IN ('{article_ids}')
                    """))

                    articles_df.to_sql(self.articles_table, conn, if_exists='append', index=False)

                    relations = []
                    for _, row in df.iterrows():
                        symbols = self.extract_stock_symbols(row['title'] + ' ' + str(row['content']))

                        for symbol in symbols:
                            relations.append({
                                'article_id': row['article_id'],
                                'symbol': symbol,
                                'relevance_score': 1.0,
                                'sentiment_type': row['sentiment_type']
                            })

                    if relations:
                        relations_df = pd.DataFrame(relations)

                        conn.execute(text(f"""
                            DELETE FROM {self.relation_table}
                            WHERE article_id IN ('{article_ids}')
                        """))

                        relations_df.to_sql(self.relation_table, conn, if_exists='append', index=False)

                    logger.info(f"âœ… [{mode}] ä¿å­˜ {len(df)} æ¡æ–°é—»ï¼Œ{len(relations)} ä¸ªè‚¡ç¥¨å…³è”")

            except Exception as e:
                logger.error(f"âŒ [{mode}] ä¿å­˜æ–°é—»å¤±è´¥: {e}")

    def get_last_date(self):
        """è·å–æœ€åé‡‡é›†çš„æ–°é—»æ—¥æœŸ"""
        for mode, engine in self.engines:
            try:
                with engine.connect() as conn:
                    # è½¬æ¢ä¸ºdateä»¥ä¾¿æ¯”è¾ƒ
                    query = text(f"SELECT DATE(MAX(publish_time)) as last_date FROM {self.articles_table}")
                    result = conn.execute(query).scalar()
                    if result:
                        # ç¡®ä¿è¿”å› date å¯¹è±¡
                        if isinstance(result, str):
                            from datetime import datetime
                            result = datetime.strptime(result, '%Y-%m-%d').date()
                        elif isinstance(result, datetime):
                            result = result.date()
                        logger.info(f"âœ… [{mode}] æœ€åé‡‡é›†æ—¥æœŸ: {result}")
                        return result
            except Exception as e:
                logger.warning(f"âš ï¸  [{mode}] è·å–æœ€åæ—¥æœŸå¤±è´¥: {e}")
                continue

        return None

    def run(self, days=None):
        """
        æ‰§è¡Œæ–°é—»é‡‡é›†ï¼ˆå¢é‡æ›´æ–°ï¼‰

        Args:
            days: é‡‡é›†æœ€è¿‘å‡ å¤©çš„æ–°é—»ï¼ˆä»…ç”¨äºé¦–æ¬¡é‡‡é›†æˆ–æ‰‹åŠ¨æŒ‡å®šï¼‰
                   None è¡¨ç¤ºå¢é‡æ›´æ–°ï¼ˆåªé‡‡é›†ç¼ºå¤±çš„æ—¥æœŸï¼‰
        """
        self.log_collection_start()
        logger.info("ğŸš€ å¼€å§‹é‡‡é›†æ–°é—»èˆ†æƒ…æ•°æ®...")

        try:
            self._health_check()
        except Exception as e:
            logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            self.log_collection_end(False, str(e))
            return

        self._init_tables()

        # ç¡®å®šé‡‡é›†æ—¥æœŸèŒƒå›´
        if days is not None:
            # æ‰‹åŠ¨æŒ‡å®šå¤©æ•°
            start_date = date.today() - timedelta(days=days-1)
            logger.info(f"ğŸ“… æ‰‹åŠ¨æ¨¡å¼ï¼šé‡‡é›†æœ€è¿‘ {days} å¤©æ–°é—»")
        else:
            # å¢é‡æ›´æ–°æ¨¡å¼ï¼šè·å–æœ€åé‡‡é›†æ—¥æœŸ
            last_date = self.get_last_date()
            if last_date:
                # ä»æœ€åæ—¥æœŸ+1å¤©å¼€å§‹é‡‡é›†
                start_date = last_date + timedelta(days=1)
                logger.info(f"ğŸ“… å¢é‡æ¨¡å¼ï¼šä» {start_date} è‡³ä»Š")
            else:
                # é¦–æ¬¡é‡‡é›†ï¼Œé‡‡é›†æœ€è¿‘3å¤©æ–°é—»
                start_date = date.today() - timedelta(days=2)
                logger.info(f"ğŸ†• é¦–æ¬¡é‡‡é›†ï¼šé‡‡é›†æœ€è¿‘3å¤©æ–°é—»")

        today = date.today()

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        if start_date > today:
            logger.info(f"âœ… æ–°é—»æ•°æ®å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ›´æ–°")
            self.log_collection_end(True, "æ•°æ®å·²æ˜¯æœ€æ–°")
            return

        # è®¡ç®—éœ€è¦é‡‡é›†çš„å¤©æ•°
        days_to_collect = (today - start_date).days + 1
        logger.info(f"ğŸ“Š éœ€è¦é‡‡é›† {days_to_collect} å¤©æ–°é—»")

        total_articles = 0
        success_count = 0
        for i in range(days_to_collect):
            current_date = start_date + timedelta(days=i)
            date_str = current_date.strftime('%Y%m%d')

            logger.info(f"ğŸ“° [{i+1}/{days_to_collect}] é‡‡é›† {date_str} çš„æ–°é—»...")

            try:
                df = self.fetch_news_em(date_str)

                if df is not None and not df.empty:
                    # æƒ…ç»ªåˆ†æ
                    df['sentiment_type'] = df.apply(
                        lambda row: self.analyze_sentiment(row['title'], row['content']),
                        axis=1
                    )

                    # ä¿å­˜æ–°é—»
                    self.save_news(df)
                    total_articles += len(df)
                    success_count += 1
                    logger.info(f"  âœ… é‡‡é›†åˆ° {len(df)} æ¡æ–°é—»")

                time.sleep(self.request_delay)

            except Exception as e:
                logger.error(f"âŒ {date_str} æ–°é—»é‡‡é›†å¤±è´¥: {e}")
                continue

        logger.info(f"ğŸ‰ æ–°é—»èˆ†æƒ…é‡‡é›†å®Œæˆï¼ŒæˆåŠŸ {success_count}/{days_to_collect} å¤©ï¼Œå…± {total_articles} æ¡æ–°é—»")
        self.log_collection_end(True, f"æˆåŠŸ {success_count}/{days_to_collect} å¤©ï¼Œå…± {total_articles} æ¡æ–°é—»")


if __name__ == "__main__":
    collector = NewsCollector()
    collector.run(days=3)
