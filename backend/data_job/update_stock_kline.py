# è·å–kçº¿æ•°æ®
# backend/data_job/update_stock_kline.py

import sys
import os
import time
import datetime
import logging
import pandas as pd
import akshare as ak
from sqlalchemy import text, inspect
from datetime import timedelta
import ssl

# ================= ğŸš‘ ç½‘ç»œæ€¥æ•‘åŒ… =================
# å¼ºåˆ¶å…³é—­ç³»ç»Ÿä»£ç† (è§£å†³ Mac å¼€ VPN å¯¼è‡´æ— æ³•è¿æ¥å›½å†…æ¥å£çš„é—®é¢˜)
for k in ['HTTP_PROXY', 'HTTPS_PROXY', 'http_proxy', 'https_proxy']:
    if k in os.environ:
        del os.environ[k]

# å¿½ç•¥ SSL è¯ä¹¦éªŒè¯
ssl._create_default_https_context = ssl._create_unverified_context
# ==========================================================

# ================= ç¯å¢ƒè·¯å¾„é€‚é… =================
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_dir = os.path.abspath(os.path.join(current_dir, ".."))
if backend_dir not in sys.path:
    sys.path.insert(0, backend_dir)

from app.core.database import get_engine

# ================= æ—¥å¿—é…ç½® =================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class StockKlineManager:
    def __init__(self):
        self.engine = get_engine()
        self.table_name = "stock_daily_prices"

    def _init_table(self):
        """åˆå§‹åŒ– daily_prices è¡¨ç»“æ„"""
        inspector = inspect(self.engine)
        if not inspector.has_table(self.table_name):
            logger.info(f"ğŸ› ï¸ [Cloud] åˆ›å»ºè¡¨ {self.table_name}...")
            with self.engine.begin() as conn:
                conn.execute(text(f"""
                    CREATE TABLE {self.table_name} (
                        symbol VARCHAR(20),
                        trade_date DATE,
                        open FLOAT,
                        close FLOAT,
                        high FLOAT,
                        low FLOAT,
                        volume FLOAT,
                        amount FLOAT,
                        pct_chg FLOAT,
                        turnover_rate FLOAT,
                        PRIMARY KEY (symbol, trade_date)
                    );
                """))
                conn.execute(text(f"CREATE INDEX IF NOT EXISTS idx_kline_symbol ON {self.table_name} (symbol);"))
                conn.execute(text(f"CREATE INDEX IF NOT EXISTS idx_kline_date ON {self.table_name} (trade_date);"))

    def get_stock_list(self):
        """è·å–è‚¡ç¥¨åå•ï¼šä¼˜å…ˆæŸ¥äº‘ç«¯æ•°æ®åº“"""
        logger.info("ğŸ“‹ æ­£åœ¨è·å–å¾…æ›´æ–°çš„è‚¡ç¥¨åå•...")
        # 1. å°è¯•ä»äº‘ç«¯ã€æ¿å—æ˜ å°„è¡¨ã€‘è¯»å–
        try:
            with self.engine.connect() as conn:
                df = pd.read_sql(text("SELECT DISTINCT symbol, name FROM stock_sector_map"), conn)
            if not df.empty:
                logger.info(f"âœ… [äº‘ç«¯] ä» stock_sector_map è·å–åˆ° {len(df)} åªè‚¡ç¥¨")
                return df[df['symbol'].astype(str).str.match(r'^(00|30|60|68)')].to_dict('records')
        except Exception: pass

        # 2. å°è¯•ä»äº‘ç«¯ã€åŸºç¡€ä¿¡æ¯è¡¨ã€‘è¯»å–
        try:
            with self.engine.connect() as conn:
                df = pd.read_sql(text("SELECT symbol, name FROM stock_info"), conn)
            if not df.empty:
                logger.info(f"âœ… [äº‘ç«¯] ä» stock_info è·å–åˆ° {len(df)} åªè‚¡ç¥¨")
                return df.to_dict('records')
        except Exception: pass

        # 3. æœ€åè”ç½‘è·å–
        for i in range(3):
            try:
                df = ak.stock_zh_a_spot_em()
                df = df[['ä»£ç ', 'åç§°']].rename(columns={'ä»£ç ':'symbol', 'åç§°':'name'})
                return df[df['symbol'].astype(str).str.match(r'^(00|30|60|68)')].to_dict('records')
            except Exception:
                time.sleep(2)
        return []

    def get_last_dates(self):
        """è·å–å¢é‡æ›´æ–°è¿›åº¦"""
        try:
            query = text(f"SELECT symbol, MAX(trade_date) as last_date FROM {self.table_name} GROUP BY symbol")
            with self.engine.connect() as conn:
                df = pd.read_sql(query, conn)
            if df.empty: return {}
            return dict(zip(df['symbol'], pd.to_datetime(df['last_date']).dt.date))
        except:
            return {}

    def _bulk_save_kline(self, df_list):
        """å†…éƒ¨è¾…åŠ©ï¼šæ‰¹é‡å­˜å…¥äº‘ç«¯ï¼Œå¹³è¡¡æ•ˆç‡ä¸ Units æ¶ˆè€—"""
        if not df_list: return
        try:
            final_df = pd.concat(df_list, ignore_index=True)
            with self.engine.begin() as conn:
                # é’ˆå¯¹ CockroachDBï¼Œmethod='multi' é…åˆåˆç†çš„ chunksize æ˜¯æœ€é«˜æ•ˆçš„å†™å…¥æ–¹å¼
                final_df.to_sql(self.table_name, conn, if_exists='append', index=False, method='multi', chunksize=1000)
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡å†™å…¥äº‘ç«¯å¤±è´¥: {e}")

    def run(self):
        """ä¸»æ‰§è¡Œå…¥å£"""
        logger.info("ğŸš€ [Kçº¿] å¯åŠ¨ä¸ªè‚¡è¡Œæƒ…äº‘ç«¯åŒæ­¥...")
        self._init_table()

        stock_list = self.get_stock_list()
        if not stock_list: return

        existing_records = self.get_last_dates()
        DEFAULT_START_DATE = "20230101"
        today = datetime.date.today()
        total = len(stock_list)

        collected_data = []
        BATCH_SIZE = 500 # ğŸ’¡ å…³é”®ï¼šæ¯ 20 åªè‚¡ç¥¨åˆå¹¶ä¸ºä¸€ä¸ªäº‹åŠ¡å†™å…¥äº‘ç«¯ï¼Œå¤§å¹…èŠ‚çœ Units

        logger.info(f"ğŸ“Š å‡†å¤‡å¤„ç† {total} åªè‚¡ç¥¨...")

        for i, stock in enumerate(stock_list):
            code = stock['symbol']
            name = stock['name']
            
            last_date = existing_records.get(code)
            if last_date:
                if last_date >= today: continue
                start_date_str = (last_date + timedelta(days=1)).strftime("%Y%m%d")
            else:
                start_date_str = DEFAULT_START_DATE

            end_date_str = today.strftime("%Y%m%d")
            if start_date_str > end_date_str: continue

            if i % 10 == 0:
                print(f"[{i+1}/{total}] åŒæ­¥è¿›åº¦: {code} {name} ...", end="\r")

            try:
                df = ak.stock_zh_a_hist(
                    symbol=code, period="daily", start_date=start_date_str, 
                    end_date=end_date_str, adjust="qfq"
                )
                
                if df is None or df.empty: continue

                rename_dict = {
                    'æ—¥æœŸ': 'trade_date', 'å¼€ç›˜': 'open', 'æ”¶ç›˜': 'close', 
                    'æœ€é«˜': 'high', 'æœ€ä½': 'low', 'æˆäº¤é‡': 'volume', 
                    'æˆäº¤é¢': 'amount', 'æ¶¨è·Œå¹…': 'pct_chg', 'æ¢æ‰‹ç‡': 'turnover_rate'
                }
                df = df.rename(columns=rename_dict)
                df['symbol'] = code
                
                for col in ['open', 'close', 'high', 'low', 'volume', 'amount', 'pct_chg', 'turnover_rate']:
                    if col not in df.columns: df[col] = None
                
                df['trade_date'] = pd.to_datetime(df['trade_date']).dt.date
                save_df = df[['symbol', 'trade_date', 'open', 'close', 'high', 'low', 'volume', 'amount', 'pct_chg', 'turnover_rate']]
                
                # ğŸ’¡ æ”¾å…¥å¾…å†™å…¥åˆ—è¡¨ï¼Œæš‚ä¸æäº¤äº‹åŠ¡
                collected_data.append(save_df)

                # ğŸ’¡ è¾¾åˆ° BATCH_SIZE æ—¶ï¼Œæ‰§è¡Œä¸€æ¬¡æ‰¹é‡å†™å…¥
                if len(collected_data) >= BATCH_SIZE:
                    self._bulk_save_kline(collected_data)
                    collected_data = [] # æ¸…ç©ºç¼“å­˜

                time.sleep(0.01) # äº‘ç«¯ç¯å¢ƒä¸‹ç¨å¾®ç»™ CPU ç•™ç‚¹ä½™åœ°

            except Exception:
                time.sleep(0.2)

        # å¤„ç†å‰©ä½™æ²¡æ»¡ BATCH_SIZE çš„æ•°æ®
        if collected_data:
            self._bulk_save_kline(collected_data)

        logger.info(f"\nâœ… ä¸ªè‚¡ K çº¿äº‘ç«¯åŒæ­¥å®Œæˆï¼")

if __name__ == "__main__":
    manager = StockKlineManager()
    manager.run()