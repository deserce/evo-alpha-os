"""
EvoAlpha OS - æ•°æ®é‡‡é›†åŸºç±»
æä¾›æ–­ç‚¹ç»­ä¼ ã€å¢é‡æ›´æ–°ã€é”™è¯¯é‡è¯•ã€è¿æ¥ç¨³å®šæ€§ç­‰é€šç”¨åŠŸèƒ½
"""

import sys
import os
import time
import json
import logging
import random
import signal
import pandas as pd
import requests
from abc import ABC, abstractmethod
from datetime import datetime, timedelta
from typing import Callable, Any, Optional
from sqlalchemy import text
from pathlib import Path
from contextlib import contextmanager

# ================= ç¯å¢ƒè·¯å¾„é€‚é… =================
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_dir = os.path.abspath(os.path.join(current_dir, ".."))
if backend_dir not in sys.path:
    sys.path.insert(0, backend_dir)

from app.core.database import get_engine


class NetworkError(Exception):
    """ç½‘ç»œé”™è¯¯"""
    pass


class ConnectionTimeout(Exception):
    """è¿æ¥è¶…æ—¶"""
    pass


class BaseCollector(ABC):
    """æ•°æ®é‡‡é›†åŸºç±» - æä¾›é€šç”¨åŠŸèƒ½å’Œè¿æ¥ç¨³å®šæ€§ä¿éšœ"""

    def __init__(self, collector_name: str,
                 request_timeout: int = 30,
                 request_delay: float = 0.5,
                 max_retries: int = 3):
        """
        åˆå§‹åŒ–é‡‡é›†å™¨

        Args:
            collector_name: é‡‡é›†å™¨åç§°ï¼ˆç”¨äºæ—¥å¿—å’Œè¿›åº¦è·Ÿè¸ªï¼‰
            request_timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            request_delay: è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        """
        self.collector_name = collector_name
        self.engine = get_engine()

        # ç½‘ç»œè¯·æ±‚é…ç½®
        self.request_timeout = request_timeout
        self.request_delay = request_delay
        self.max_retries = max_retries
        self.session = self._create_session()

        # è¿›åº¦æ–‡ä»¶è·¯å¾„
        self.progress_dir = Path(backend_dir) / "data" / "collection_progress"
        self.progress_dir.mkdir(parents=True, exist_ok=True)
        self.progress_file = self.progress_dir / f"{collector_name}.json"

        # åŠ è½½è¿›åº¦
        self.progress = self._load_progress()

        # æ—¥å¿—é…ç½®
        self.logger = logging.getLogger(f"collector.{collector_name}")

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_requests": 0,
            "failed_requests": 0,
            "retry_count": 0,
            "timeout_count": 0
        }

    def _create_session(self) -> requests.Session:
        """
        åˆ›å»ºå¸¦è¿æ¥æ± çš„Session

        Returns:
            é…ç½®å¥½çš„Sessionå¯¹è±¡
        """
        session = requests.Session()

        # è¿æ¥æ± é…ç½®
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=10,  # è¿æ¥æ± æ•°é‡
            pool_maxsize=20,      # æ¯ä¸ªæ± çš„æœ€å¤§è¿æ¥æ•°
            max_retries=0         # ç¦ç”¨è‡ªåŠ¨é‡è¯•ï¼ˆæˆ‘ä»¬æ‰‹åŠ¨æ§åˆ¶ï¼‰
        )
        session.mount('http://', adapter)
        session.mount('https://', adapter)

        # è®¾ç½®è¶…æ—¶
        session.timeout = self.request_timeout

        return session

    @contextmanager
    def _timeout_context(self, timeout_seconds: int):
        """
        è¶…æ—¶ä¸Šä¸‹æ–‡ç®¡ç†å™¨

        Args:
            timeout_seconds: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        def timeout_handler(signum, frame):
            raise ConnectionTimeout(f"æ“ä½œè¶…æ—¶ï¼ˆ{timeout_seconds}ç§’ï¼‰")

        # è®¾ç½®ä¿¡å·å¤„ç†å™¨
        old_handler = signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(timeout_seconds)

        try:
            yield
        finally:
            # æ¢å¤åŸæ¥çš„å¤„ç†å™¨
            signal.alarm(0)
            signal.signal(signal.SIGALRM, old_handler)

    def _check_network_connection(self) -> bool:
        """
        æ£€æŸ¥ç½‘ç»œè¿æ¥

        Returns:
            ç½‘ç»œæ˜¯å¦å¯ç”¨
        """
        try:
            # å°è¯•è¿æ¥ç™¾åº¦æ£€æµ‹ç½‘ç»œ
            response = self.session.get(
                "https://www.baidu.com",
                timeout=5
            )
            return response.status_code == 200
        except:
            return False

    def _health_check(self):
        """è¿æ¥å¥åº·æ£€æŸ¥"""
        if not self._check_network_connection():
            raise NetworkError("ç½‘ç»œè¿æ¥ä¸å¯ç”¨")

        # æµ‹è¯•æ•°æ®åº“è¿æ¥
        try:
            with self.engine.connect() as conn:
                conn.execute(text("SELECT 1"))
        except Exception as e:
            raise NetworkError(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")

    def _load_progress(self) -> dict:
        """åŠ è½½é‡‡é›†è¿›åº¦"""
        if self.progress_file.exists():
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                self.logger.warning(f"åŠ è½½è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")
        return {
            "last_update": None,
            "last_symbol": None,
            "failed_items": [],
            "collection_count": 0,
            "last_success_time": None
        }

    def _save_progress(self):
        """ä¿å­˜é‡‡é›†è¿›åº¦"""
        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(self.progress, f, indent=2, ensure_ascii=False)
        except Exception as e:
            self.logger.warning(f"ä¿å­˜è¿›åº¦æ–‡ä»¶å¤±è´¥: {e}")

    def _retry_call(self, func, max_retries=None, delay=None,
                    exponential_backoff=True, **kwargs):
        """
        å¢å¼ºçš„é‡è¯•æœºåˆ¶ï¼ˆæ”¯æŒæŒ‡æ•°é€€é¿å’ŒæŠ–åŠ¨ï¼‰

        Args:
            func: è¦æ‰§è¡Œçš„å‡½æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ä½¿ç”¨self.max_retriesï¼‰
            delay: åˆå§‹å»¶è¿Ÿï¼ˆç§’ï¼‰
            exponential_backoff: æ˜¯å¦ä½¿ç”¨æŒ‡æ•°é€€é¿
            **kwargs: å‡½æ•°å‚æ•°

        Returns:
            å‡½æ•°æ‰§è¡Œç»“æœï¼Œå¤±è´¥è¿”å›None
        """
        if max_retries is None:
            max_retries = self.max_retries
        if delay is None:
            delay = self.request_delay

        last_error = None

        for attempt in range(max_retries):
            try:
                self.stats["total_requests"] += 1

                # å¥åº·æ£€æŸ¥
                if attempt > 0:
                    self._health_check()

                # æ‰§è¡Œå‡½æ•°
                result = func(**kwargs)

                # è¯·æ±‚æˆåŠŸåæ·»åŠ éšæœºå»¶è¿Ÿï¼ˆé¿å…è¢«å°ï¼‰
                if result is not None:
                    jitter = random.uniform(0, 0.3)  # 0-0.3ç§’éšæœºæŠ–åŠ¨
                    time.sleep(self.request_delay + jitter)

                return result

            except (ConnectionTimeout, requests.exceptions.Timeout) as e:
                self.stats["timeout_count"] += 1
                last_error = e
                self.logger.warning(f"è¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries})")

            except (requests.exceptions.ConnectionError,
                   requests.exceptions.RequestException) as e:
                self.stats["failed_requests"] += 1
                last_error = e
                self.logger.warning(f"ç½‘ç»œé”™è¯¯ (å°è¯• {attempt + 1}/{max_retries}): {e}")

            except Exception as e:
                last_error = e
                if attempt < max_retries - 1:
                    self.logger.warning(f"è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                else:
                    self.logger.error(f"è¯·æ±‚å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                    self.stats["failed_requests"] += 1
                    return None

            # è®¡ç®—ç­‰å¾…æ—¶é—´
            if attempt < max_retries - 1:
                if exponential_backoff:
                    # æŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨
                    wait_time = delay * (2 ** attempt) + random.uniform(0, 1)
                else:
                    wait_time = delay + random.uniform(0, 0.5)

                self.stats["retry_count"] += 1
                self.logger.info(f"ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                time.sleep(wait_time)

        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        self.logger.error(f"è¯·æ±‚å¤±è´¥: {last_error}")
        return None

    def _retry_with_fallback(self, primary_func: Callable,
                            fallback_func: Callable,
                            **kwargs) -> Optional[Any]:
        """
        å¸¦é™çº§ç­–ç•¥çš„é‡è¯•ï¼ˆä¸»æ¥å£å¤±è´¥æ—¶å°è¯•å¤‡ç”¨æ¥å£ï¼‰

        Args:
            primary_func: ä¸»å‡½æ•°
            fallback_func: å¤‡ç”¨å‡½æ•°
            **kwargs: å‡½æ•°å‚æ•°

        Returns:
            å‡½æ•°æ‰§è¡Œç»“æœ
        """
        # å…ˆå°è¯•ä¸»å‡½æ•°
        result = self._retry_call(primary_func, max_retries=2, **kwargs)
        if result is not None:
            return result

        # ä¸»å‡½æ•°å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨å‡½æ•°
        self.logger.warning("ä¸»æ¥å£å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ¥å£...")
        return self._retry_call(fallback_func, max_retries=2, **kwargs)

    def get_last_update_date(self) -> datetime:
        """
        è·å–æœ€åæ›´æ–°æ—¥æœŸï¼ˆç”¨äºå¢é‡æ›´æ–°ï¼‰

        Returns:
            æœ€åæ›´æ–°æ—¥æœŸï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›90å¤©å‰
        """
        last_update = self.progress.get("last_update")
        if last_update:
            try:
                return datetime.fromisoformat(last_update)
            except:
                pass

        # é»˜è®¤è¿”å›90å¤©å‰
        return datetime.now() - timedelta(days=90)

    def update_progress(self, **kwargs):
        """
        æ›´æ–°é‡‡é›†è¿›åº¦

        Args:
            **kwargs: è¦æ›´æ–°çš„è¿›åº¦å­—æ®µ
        """
        self.progress.update(kwargs)
        self._save_progress()

    def log_collection_start(self):
        """è®°å½•é‡‡é›†å¼€å§‹"""
        self.logger.info(f"ğŸš€ å¼€å§‹é‡‡é›† [{self.collector_name}]")
        self.progress["collection_start_time"] = datetime.now().isoformat()
        self._save_progress()

    def log_collection_end(self, success: bool, message: str = ""):
        """
        è®°å½•é‡‡é›†ç»“æŸ

        Args:
            success: æ˜¯å¦æˆåŠŸ
            message: é™„åŠ ä¿¡æ¯
        """
        if success:
            self.logger.info(f"âœ… é‡‡é›†å®Œæˆ [{self.collector_name}] {message}")
            self.progress["last_success_time"] = datetime.now().isoformat()
        else:
            self.logger.error(f"âŒ é‡‡é›†å¤±è´¥ [{self.collector_name}] {message}")

        self.progress["collection_end_time"] = datetime.now().isoformat()
        self.progress["collection_success"] = success
        self._save_progress()

    def get_collection_statistics(self) -> dict:
        """
        è·å–é‡‡é›†ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        return {
            "collector_name": self.collector_name,
            "last_update": self.progress.get("last_update"),
            "collection_count": self.progress.get("collection_count", 0),
            "last_success_time": self.progress.get("last_success_time"),
            "failed_items_count": len(self.progress.get("failed_items", []))
        }

    def save_with_deduplication(self, df: pd.DataFrame, table_name: str,
                                key_columns: list, date_column: str = None):
        """
        ä¿å­˜æ•°æ®å¹¶å»é‡ï¼ˆå¢é‡æ›´æ–°ï¼‰

        Args:
            df: è¦ä¿å­˜çš„æ•°æ®
            table_name: è¡¨å
            key_columns: ä¸»é”®åˆ—åˆ—è¡¨
            date_column: æ—¥æœŸåˆ—åï¼ˆç”¨äºå¢é‡æ›´æ–°ï¼‰
        """
        if df.empty:
            self.logger.warning("æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜")
            return 0

        try:
            with self.engine.begin() as conn:
                # å¦‚æœæœ‰æ—¥æœŸåˆ—ï¼Œåªåˆ é™¤æ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
                if date_column and date_column in df.columns:
                    min_date = df[date_column].min()
                    max_date = df[date_column].max()

                    # æ„å»ºåˆ é™¤æ¡ä»¶
                    key_condition = " AND ".join([f"{col} = :{col}" for col in key_columns])

                    # åˆ é™¤é‡å¤æ•°æ®
                    for _, row in df.iterrows():
                        params = {col: row[col] for col in key_columns}
                        params.update({
                            "min_date": min_date,
                            "max_date": max_date
                        })
                        conn.execute(text(f"""
                            DELETE FROM {table_name}
                            WHERE {key_condition}
                            AND {date_column} BETWEEN :min_date AND :max_date
                        """), params)
                else:
                    # åˆ é™¤æ‰€æœ‰ä¸»é”®é‡å¤çš„æ•°æ®
                    for _, row in df.iterrows():
                        params = {col: row[col] for col in key_columns}
                        conn.execute(text(f"""
                            DELETE FROM {table_name}
                            WHERE {" AND ".join([f"{col} = :{col}" for col in key_columns])}
                        """), params)

                # æ’å…¥æ–°æ•°æ®ï¼ˆä½¿ç”¨ chunksize é¿å… SQLite å˜é‡é™åˆ¶ï¼‰
                df.to_sql(table_name, conn, if_exists='append', index=False,
                         method='multi', chunksize=100)

                inserted_count = len(df)
                self.progress["collection_count"] = self.progress.get("collection_count", 0) + inserted_count
                self._save_progress()

                return inserted_count

        except Exception as e:
            self.logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            raise

    def clean_old_data(self, table_name: str, date_column: str,
                      keep_days: int = 365):
        """
        æ¸…ç†æ—§æ•°æ®ï¼ˆä¿ç•™æœ€è¿‘Nå¤©ï¼‰

        Args:
            table_name: è¡¨å
            date_column: æ—¥æœŸåˆ—å
            keep_days: ä¿ç•™å¤©æ•°
        """
        try:
            cutoff_date = datetime.now() - timedelta(days=keep_days)

            with self.engine.begin() as conn:
                result = conn.execute(text(f"""
                    DELETE FROM {table_name}
                    WHERE {date_column} < :cutoff_date
                """), {"cutoff_date": cutoff_date})

                deleted_count = result.rowcount
                if deleted_count > 0:
                    self.logger.info(f"æ¸…ç†äº† {deleted_count} æ¡æ—§æ•°æ®ï¼ˆ{table_name}ï¼‰")

                return deleted_count

        except Exception as e:
            self.logger.error(f"æ¸…ç†æ—§æ•°æ®å¤±è´¥: {e}")
            return 0

    @abstractmethod
    def run(self):
        """
        æ‰§è¡Œé‡‡é›†ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        pass


class BatchCollector(BaseCollector):
    """æ‰¹é‡æ•°æ®é‡‡é›†åŸºç±» - æ”¯æŒåˆ†æ‰¹é‡‡é›†å’Œæ–­ç‚¹ç»­ä¼ """

    def __init__(self, collector_name: str, batch_size: int = 100):
        """
        åˆå§‹åŒ–æ‰¹é‡é‡‡é›†å™¨

        Args:
            collector_name: é‡‡é›†å™¨åç§°
            batch_size: æ¯æ‰¹å¤„ç†æ•°é‡
        """
        super().__init__(collector_name)
        self.batch_size = batch_size

    @abstractmethod
    def get_item_list(self) -> list:
        """
        è·å–è¦é‡‡é›†çš„é¡¹ç›®åˆ—è¡¨ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰

        Returns:
            é¡¹ç›®åˆ—è¡¨
        """
        pass

    @abstractmethod
    def process_item(self, item) -> pd.DataFrame:
        """
        å¤„ç†å•ä¸ªé¡¹ç›®ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰

        Args:
            item: è¦å¤„ç†çš„é¡¹ç›®

        Returns:
            å¤„ç†åçš„æ•°æ®
        """
        pass

    @abstractmethod
    def save_item_data(self, item, df: pd.DataFrame):
        """
        ä¿å­˜å•ä¸ªé¡¹ç›®çš„æ•°æ®ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰

        Args:
            item: é¡¹ç›®
            df: æ•°æ®
        """
        pass

    def run(self, resume: bool = True):
        """
        æ‰§è¡Œæ‰¹é‡é‡‡é›†

        Args:
            resume: æ˜¯å¦ä»æ–­ç‚¹ç»§ç»­

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        self.log_collection_start()

        try:
            # è·å–è¦å¤„ç†çš„é¡¹ç›®åˆ—è¡¨
            items = self.get_item_list()
            if not items:
                self.logger.warning("æ²¡æœ‰è¦å¤„ç†çš„é¡¹ç›®")
                self.log_collection_end(True, "æ— é¡¹ç›®éœ€è¦å¤„ç†")
                return True

            total = len(items)
            self.logger.info(f"å…± {total} ä¸ªé¡¹ç›®éœ€è¦å¤„ç†")

            # è·å–ä¸Šæ¬¡å¤„ç†çš„ä½ç½®
            start_index = 0
            if resume:
                last_item = self.progress.get("last_item")
                if last_item in items:
                    start_index = items.index(last_item) + 1
                    self.logger.info(f"ä»ç¬¬ {start_index + 1} ä¸ªé¡¹ç›®ç»§ç»­...")

            # å¤„ç†æ¯ä¸ªé¡¹ç›®
            success_count = 0
            failed_items = []

            for i in range(start_index, total):
                item = items[i]

                try:
                    self.logger.info(f"å¤„ç† [{i + 1}/{total}]: {item}")

                    # å¤„ç†é¡¹ç›®
                    df = self._retry_call(
                        lambda: self.process_item(item),
                        max_retries=3
                    )

                    if df is not None and not df.empty:
                        # ä¿å­˜æ•°æ®
                        self.save_item_data(item, df)
                        success_count += 1

                        # æ›´æ–°è¿›åº¦
                        self.update_progress(
                            last_item=str(item),
                            processed_count=i + 1,
                            success_count=success_count
                        )

                    # é¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(0.5)

                except Exception as e:
                    self.logger.error(f"å¤„ç†å¤±è´¥ [{item}]: {e}")
                    failed_items.append(str(item))

                    # è®°å½•å¤±è´¥é¡¹
                    failed_list = self.progress.get("failed_items", [])
                    failed_list.append({
                        "item": str(item),
                        "time": datetime.now().isoformat(),
                        "error": str(e)
                    })
                    self.update_progress(failed_items=failed_list[-100:])  # åªä¿ç•™æœ€è¿‘100ä¸ª

            # å®Œæˆ
            self.update_progress(last_update=datetime.now().isoformat())
            message = f"æˆåŠŸ: {success_count}/{total}"
            if failed_items:
                message += f", å¤±è´¥: {len(failed_items)}"

            self.log_collection_end(True, message)
            return True

        except Exception as e:
            self.log_collection_end(False, str(e))
            return False
